{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  #biblioteca regex python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexAnalyzer():  #classe LexAnalyzer\n",
    "    def __init__(self, filename: str, keywords: dict):  #construída com 2 atributos\n",
    "        self.filename = filename  #Nome do arquivo, para a leitura\n",
    "        self.keywords = keywords  #keywords recebe um dicionário com as palavras reservadas\n",
    "\n",
    "    def analyzer(self):  #método que fará a análise lexica\n",
    "        with open(self.filename) as file:  #abre o arquivo e o nomeia como file\n",
    "            tolines = file.readlines()  #variável tolines converte todas as linhas em elementos de uma lista\n",
    "            linhas = 1  #iniciando a variável de linhas em 1, para contagem de linhas e impressão nas tuplas\n",
    "            listona = []\n",
    "            for line in tolines:  #percorrendo uma line na lista de linhas\n",
    "                lexema = []  #lista que vai conter a palavra\n",
    "                tupla = []  #lista que vai receber as tuplas de cada linha\n",
    "                coment = 0\n",
    "                for letter in line:  #para cada letra dentro da linha\n",
    "                    if letter == '{':\n",
    "                        coment = 1\n",
    "                    if coment == 0:\n",
    "                        if letter != \" \" and letter != '\\n' and letter != '\\t':  #caso caso não seja um espaço, quebra de linha ou tabulação\n",
    "                            lexema.append(letter)  #adiciona a letra ao fim da lista lexema\n",
    "                        elif letter == \" \" or letter == '\\n':  #encontrando o espaço, verifica-se a palavra\n",
    "                            lex = \"\".join(lexema)  #converte-se a lista lexema numa string lex\n",
    "                            if lex in self.keywords:  #busca se a palavra está contida no dicionário\n",
    "                                # print(f'({self.keywords[lex]}, {linhas})\\n')  #se está, printa o valor (token), e a linha respectiva\n",
    "                                tupla.append(f'({self.keywords[lex]}, {linhas})')  #adiciona a lista de tuplas/linha a tupla gerada\n",
    "                            elif re.match(r\"[A-Za-z]+[A-Z|a-z|0-9]*\", lex):  #caso esteja incluso na expressão de id, retorna id e a linha respectiva\n",
    "                                tupla.append(f\"(id, {linhas})\")  #adiciona a lista de tuplas/linha a tupla gerada\n",
    "                                # print(f\"(id, {linhas})\\n\")\n",
    "                            elif re.match(r\"\\d+\", lex):  #caso seja um dígito, retorna Dígito e a linha respectiva\n",
    "                                # print(f\"(Digito, {linhas})\\n\")\n",
    "                                tupla.append(f\"(Digito, {linhas})\")  #adiciona a lista de tuplas/linha a tupla gerada\n",
    "                            else:\n",
    "                                pass  #caso não for, continua (provavelmente seria bom tratar erros lexicos\n",
    "                            lexema.clear()  #limpa a lista de lexemas para a próxima palavra\n",
    "                    if letter == '}':\n",
    "                        coment = 0\n",
    "                if linhas == len(tolines):  #caso esteja na ultima palavra do arquivo, repete\n",
    "                    if tolines[-1][-1] in self.keywords:\n",
    "                        tupla.append(f'({self.keywords[tolines[-1][-1]]}, {len(tolines)})')\n",
    "                    elif re.match(r\"[A-Za-z]+[A-Za-z0-9]*\", tolines[-1][-1]):  #caso esteja incluso na expressão de id, retorna id e a linha respectiva\n",
    "                        tupla.append(f'(id, {len(tolines)})')\n",
    "                    elif re.match(r\"\\d+\", tolines[-1][-1]):  #caso seja um dígito, retorna Dígito e a linha respectiva\n",
    "                        tupla.append(f'(Digito, {len(tolines)})')\n",
    "                tuplas = tupla.copy()  #lista tuplas copia o conteúdo da lista tupla (por se tratar de endereços, precisa-se do método copy)\n",
    "                listona.append(tuplas)  #adicionando a tupla de cada linha a uma lista que vai conter todas as tuplas geradas, por linha\n",
    "                tupla.clear()  #limpa a lista de tuplas, para passar pra próxima linha\n",
    "                linhas+=1  #adiciona uma linha\n",
    "            print(f\"Total de linhas: {len(tolines)}\")  #ao fim, printa o número de linhas do arquivo\n",
    "            for i, list in enumerate (listona):  #iterando sobre a lista com todas as tuplas por linha, mostrando a linha e as respectivas tuplas\n",
    "                print(f'Linha: {i+1}: {list}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"teste.txt\"\n",
    "\n",
    "keywords = {\n",
    "\"programa\": \"prog\",\n",
    "\"var\": \"var\",\n",
    "\"inteiro\": \"int\",\n",
    "\"procedimento\": \"proced\",\n",
    "\"funcao\": \"func\",\n",
    "\"inicio\": \"start\",\n",
    "\"fim\": \"end\",\n",
    "\"se\": \"if\",\n",
    "\"entao\": \"then\",\n",
    "\"senao\": \"else\",\n",
    "\"enquanto\": \"while\",\n",
    "\"faca\": \"do\",\n",
    "\"leia\": \"read\",\n",
    "\"escreva\": \"write\",\n",
    "\"<>\": \"diferente\",\n",
    "\"=\": \"igual\",\n",
    "\"<\": \"menorq\",\n",
    "\"<=\": \"menorouigualq\",\n",
    "\">\": \"maiorq\",\n",
    "\">=\": \"maiorouigualq\",\n",
    "\":=\": \"atrib\",\n",
    "\"+\": \"plus\",\n",
    "\"-\": \"minus\",\n",
    "\"ou\": \"or\",\n",
    "\"e\": \"and\",\n",
    "\"*\": \"mult\",\n",
    "\"div\": \"divide\",\n",
    "\"verdadeiro\": \"true\",\n",
    "\"falso\": \"false\",\n",
    "\"nao\": \"not\",\n",
    "\",\": \"colon\",\n",
    "\":\": \"2dots\",\n",
    "\";\": \"semicolon\",\n",
    "\".\": \"dot\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de linhas: 19\n",
      "Linha: 1: ['(prog, 1)', '(id, 1)', '(semicolon, 1)']\n",
      "Linha: 2: ['(var, 2)', '(id, 2)', '(2dots, 2)', '(int, 2)', '(semicolon, 2)']\n",
      "Linha: 3: ['(id, 3)', '(colon, 3)', '(id, 3)', '(colon, 3)', '(id, 3)', '(2dots, 3)', '(int, 3)', '(semicolon, 3)']\n",
      "Linha: 4: ['(start, 4)']\n",
      "Linha: 5: ['(while, 5)', '(id, 5)', '(diferente, 5)', '(Digito, 5)', '(do, 5)']\n",
      "Linha: 6: ['(start, 6)']\n",
      "Linha: 7: ['(read, 7)', '(id, 7)', '(semicolon, 7)']\n",
      "Linha: 8: ['(read, 8)', '(id, 8)', '(semicolon, 8)']\n",
      "Linha: 9: ['(read, 9)', '(id, 9)', '(semicolon, 9)', '(semicolon, 9)']\n",
      "Linha: 10: ['(id, 10)', '(atrib, 10)', '(Digito, 10)', '(semicolon, 10)']\n",
      "Linha: 11: ['(id, 11)', '(atrib, 11)', '(Digito, 11)', '(semicolon, 11)']\n",
      "Linha: 12: ['(while, 12)', '(id, 12)', '(menorouigualq, 12)', '(id, 12)', '(do, 12)']\n",
      "Linha: 13: ['(start, 13)']\n",
      "Linha: 14: ['(id, 14)', '(atrib, 14)', '(id, 14)', '(mult, 14)', '(Digito, 14)', '(plus, 14)', '(id, 14)', '(semicolon, 14)']\n",
      "Linha: 15: ['(id, 15)', '(atrib, 15)', '(id, 15)', '(plus, 15)', '(Digito, 15)', '(semicolon, 15)']\n",
      "Linha: 16: ['(end, 16)']\n",
      "Linha: 17: ['(write, 17)', '(id, 17)', '(semicolon, 17)']\n",
      "Linha: 18: ['(end, 18)']\n",
      "Linha: 19: ['(end, 19)', '(dot, 19)']\n"
     ]
    }
   ],
   "source": [
    "Analyzer = LexAnalyzer(filename, keywords)  #cria o objeto LexAnalyzer na variável Analyzer\n",
    "Analyzer.analyzer()  #chama o método analyzer para fazer a análise lexica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tolines[-1][-1] in self.keywords:\n",
    "    print(f'Linha: {len(tolines)}: ({self.keywords[tolines[-1][-1]]}, {len(tolines)})')\n",
    "elif re.match(r\"[A-Za-z]+[A-Z|a-z|0-9]*$[\\S]*\", tolines[-1][-1]):  #caso esteja incluso na expressão de id, retorna id e a linha respectiva\n",
    "    # tupla.append(f\"(id, {linhas})\")  #adiciona a lista de tuplas/linha a tupla gerada\n",
    "    print(f'Linha: {len(tolines)}: (id, {len(tolines)})')\n",
    "    # print(f\"(id, {linhas})\\n\")\n",
    "elif re.match(r\"\\d+\", tolines[-1][-1]):  #caso seja um dígito, retorna Dígito e a linha respectiva\n",
    "    print(f'Linha: {len(tolines)}: (Digito, {len(tolines)})')\n",
    "    # tupla.append(f\"(Digito, {linhas})\")  #adiciona a lista de tuplas/linha a tupla gerada"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
